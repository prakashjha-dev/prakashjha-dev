# 👋 Hi, I'm Prakash Jha  

🎓 **B.Tech Graduate (2024)** | **.NET Developer (1+ year industry experience)**  
📊 Pivoting into **Data Engineering & AI/ML-ready Pipelines**  
🚀 On a **90-day journey** to build hands-on projects in **ETL, Cloud, Streaming, and Applied ML Integration** — documenting it here as a public learning log.  

---

## 🌟 Why This Journey?  
I’ve worked with .NET and SQL-based systems in production, but I want to expand into **end-to-end Data Engineering** with a strong foundation in **cloud, pipelines, and scalable workflows**.  
This journey is my commitment to:  
- Learning **industry-standard tools** used by Data Engineers.  
- Building a **portfolio of real-world projects** that recruiters value.  
- Exploring how **Machine Learning can be embedded** into production pipelines.  

---

## 🛠️ Technical Skillset  

**Languages:** Python | SQL | C# | C++  
**Databases:** SQL Server | PostgreSQL | MySQL  
**Data Tools:** Pandas | NumPy | Airflow | Spark | Kafka | Jupyter  
**Cloud & DevOps:** AWS (S3, Glue, Redshift) | GCP (BigQuery) | Docker | Git  
**Other:** APIs | ETL workflows | Data Modeling | Monitoring & Validation  

---

## 🚀 90-Day Roadmap (with Projects)  

These are the projects I’ll complete step by step in this journey. Each one builds on the last to demonstrate **recruiter-ready, end-to-end DE + ML skills**:  

1. **CSV & API Data ETL Pipeline**  
   - Build a pipeline to extract from CSV + REST APIs, transform with Pandas, and load into PostgreSQL.  
   - *Tech:* Python, Pandas, PostgreSQL  

2. **Data Warehouse & ETL Automation**  
   - Design a **star schema warehouse** and automate daily batch ingestion with **Airflow**.  
   - *Tech:* Airflow, PostgreSQL, SQL  

3. **Real-time Streaming Data Pipeline**  
   - Ingest streaming data (simulated API or log events) using **Kafka + Spark Streaming**, push into a database.  
   - *Tech:* Kafka, Spark, Python  

4. **Automated Data Quality & Validation Framework**  
   - Add validation checks for schema, nulls, anomalies before data loads.  
   - *Tech:* Python, Great Expectations, SQL  

5. **Cloud-based Lakehouse Mini Project**  
   - Store raw + processed data in **AWS S3**, transform with **Glue**, and query with **Redshift/BigQuery**.  
   - *Tech:* AWS/GCP stack  

6. **Data Pipeline with ML Model Integration**  
   - Extend a pipeline to include a **predictive ML model** (e.g., sales forecasting or churn prediction).  
   - *Tech:* Python, scikit-learn, Airflow  

7. **Portfolio Dashboard**  
   - Visualize processed data with a dashboard (Plotly/Dash or Power BI) to demonstrate **end-to-end workflow**.  
   - *Tech:* Dash, Power BI  

---

## 📊 Progress Tracker (90 Days)  

```mermaid
gantt
    title 90-Day Data Engineering Journey
    dateFormat  YYYY-MM-DD
    section Phase 1 - Foundations
    CSV/API ETL Pipeline        :done,    des1, 2025-09-22, 7d
    Warehouse & Airflow ETL     :active,  des2, 2025-09-29, 7d
    section Phase 2 - Advanced Pipelines
    Streaming with Kafka/Spark  :des3, 2025-10-06, 7d
    Data Quality Framework      :des4, 2025-10-13, 7d
    section Phase 3 - Cloud & ML
    Cloud Lakehouse             :des5, 2025-10-20, 10d
    Pipeline + ML Integration   :des6, 2025-10-30, 14d
    Portfolio Dashboard         :des7, 2025-11-13, 7d
