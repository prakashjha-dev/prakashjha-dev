# 👋 Hi, I'm Prakash Jha   

🎓 **B.Tech Graduate (2024)** | **.NET Developer (1+ year industry experience)**  
📊 Pivoting into **Data Engineering & AI/ML-ready Pipelines**  
🚀 On a **90-day journey** to build hands-on projects in **ETL, Cloud, Streaming, and Applied ML Integration** — documenting it here as a public learning log.  

---

## 📫 Connect with Me  

<div align="center">
  <a href="https://www.linkedin.com/in/your-linkedin/">
    <img src="https://img.shields.io/badge/LinkedIn-Prakash%20Jha-blue?style=for-the-badge&logo=linkedin&logoColor=white" alt="LinkedIn" height="40"/>
  </a>
  <a href="https://github.com/prakashjha-dev">
    <img src="https://img.shields.io/badge/GitHub-PrakashJha-black?style=for-the-badge&logo=github&logoColor=white" alt="GitHub" height="40"/>
  </a>
  <a href="mailto:prakashjha050201@gmail.com">
    <img src="https://img.shields.io/badge/Email-Contact%20Me-red?style=for-the-badge&logo=gmail&logoColor=white" alt="Email" height="40"/>
  </a>
</div>

---

## 🌟 Why This Journey?  
I’ve worked with .NET and SQL-based systems in production, but I want to expand into **end-to-end Data Engineering** with a strong foundation in **cloud, pipelines, and scalable workflows**.  
This journey is my commitment to:  
- Learning **industry-standard tools** used by Data Engineers.  
- Building a **portfolio of real-world projects** that recruiters value.  
- Exploring how **Machine Learning can be embedded** into production pipelines.  

---

## 🛠️ Technical Skillset  

**Languages:** Python | SQL | C# | C++  
**Databases:** SQL Server | PostgreSQL | MySQL  
**Data Tools:** Pandas | NumPy | Airflow | Spark | Kafka | Jupyter  
**Cloud & DevOps:** AWS (S3, Glue, Redshift) | GCP (BigQuery) | Docker | Git  
**Other:** APIs | ETL Workflows | Data Modeling | Monitoring & Validation  

---

## 🚀 90-Day Roadmap (with Projects)  

| # | Project Name | Description | Tech Stack | Duration |
|---|--------------|-------------|------------|---------|
| 1 | CSV & API Data ETL Pipeline | Extract from CSV + REST APIs, transform with Pandas, load into PostgreSQL | Python, Pandas, PostgreSQL | 7 days |
| 2 | Data Warehouse & ETL Automation | Design a star schema warehouse, automate daily batch ingestion with Airflow | Airflow, PostgreSQL, SQL | 7 days |
| 3 | Real-time Streaming Data Pipeline | Ingest streaming data (simulated API/log events) with Kafka + Spark Streaming | Kafka, Spark, Python | 7 days |
| 4 | Automated Data Quality & Validation Framework | Add validation checks for schema, nulls, anomalies before data loads | Python, Great Expectations, SQL | 7 days |
| 5 | Cloud-based Lakehouse Mini Project | Store raw + processed data in AWS S3, transform with Glue, query with Redshift/BigQuery | AWS/GCP stack | 10 days |
| 6 | Data Pipeline with ML Model Integration | Extend a pipeline to include predictive ML model (sales forecasting/churn prediction) | Python, scikit-learn, Airflow | 14 days |
| 7 | Portfolio Dashboard | Visualize processed data with a dashboard to demonstrate end-to-end workflow | Dash, Power BI | 7 days |

---

## 📊 Progress Tracker (90 Days)  

<div align="center">

```mermaid
gantt
    title 90-Day Data Engineering Journey
    dateFormat  YYYY-MM-DD
    section Phase 1 - Foundations
    CSV/API ETL Pipeline        :done,    des1, 2025-09-22, 7d
    Warehouse & Airflow ETL     :active,  des2, 2025-09-29, 7d
    section Phase 2 - Advanced Pipelines
    Streaming with Kafka/Spark  :des3, 2025-10-06, 7d
    Data Quality Framework      :des4, 2025-10-13, 7d
    section Phase 3 - Cloud & ML
    Cloud Lakehouse             :des5, 2025-10-20, 10d
    Pipeline + ML Integration   :des6, 2025-10-30, 14d
    Portfolio Dashboard         :des7, 2025-11-13, 7d

